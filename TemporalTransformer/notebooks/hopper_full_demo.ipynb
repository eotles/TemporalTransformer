{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running develop\n",
      "running egg_info\n",
      "writing TemporalTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to TemporalTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to TemporalTransformer.egg-info/top_level.txt\n",
      "reading manifest file 'TemporalTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'TemporalTransformer.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "Creating /opt/conda/lib/python3.8/site-packages/TemporalTransformer.egg-link (link to .)\n",
      "TemporalTransformer 0.0.1 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /home/jovyan/work/TemporalTransformer/notebooks\n",
      "Processing dependencies for TemporalTransformer==0.0.1\n",
      "Finished processing dependencies for TemporalTransformer==0.0.1\n"
     ]
    }
   ],
   "source": [
    "!python /home/jovyan/work/setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.chdir('/home/jovyan/work/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp38-cp38-manylinux2010_x86_64.whl (320.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 320.5 MB 4.6 kB/s  eta 0:00:01  |                                | 624 kB 3.5 MB/s eta 0:01:31     |                                | 901 kB 3.5 MB/s eta 0:01:31     |█████▏                          | 51.2 MB 21.4 MB/s eta 0:00:13     |████████▌                       | 84.6 MB 8.0 MB/s eta 0:00:30     |████████████▏                   | 121.8 MB 8.4 MB/s eta 0:00:24     |█████████████▎                  | 132.9 MB 11.4 MB/s eta 0:00:17     |█████████████▋                  | 136.4 MB 9.3 MB/s eta 0:00:20     |███████████████▍                | 153.8 MB 10.3 MB/s eta 0:00:17     |███████████████▋                | 155.9 MB 7.1 MB/s eta 0:00:24     |█████████████████▍              | 174.4 MB 13.1 MB/s eta 0:00:12     |████████████████████▊           | 207.2 MB 12.7 MB/s eta 0:00:09     |████████████████████▉           | 208.2 MB 12.7 MB/s eta 0:00:09     |██████████████████████▌         | 225.4 MB 4.2 MB/s eta 0:00:23     |█████████████████████████       | 251.2 MB 10.5 MB/s eta 0:00:07     |█████████████████████████▍      | 254.6 MB 10.5 MB/s eta 0:00:07 | 268.8 MB 5.6 MB/s eta 0:00:10     |███████████████████████████     | 270.5 MB 8.6 MB/s eta 0:00:06████████▊   | 287.4 MB 7.8 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 5.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 395 kB/s  eta 0:00:01     |██████████████████████████      | 16.7 MB 20.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.35.1)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 21.5 MB/s eta 0:00:01     |█████████████████████████▊      | 3.0 MB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.12.4)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 7.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.22.1-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 22.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.6.0.post20200917)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 16.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 4.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.10)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 6.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.0.1)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=9834cc5ae4cb7aee81013755c6ed3ca172109943f148a044105c067cd91c3450\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=81735 sha256=81157bcc2f1f7d0f4a32b24bbf5f151bdffdd9676634e15b3d44e341296fee9d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: absl-py, google-pasta, astunparse, numpy, termcolor, wrapt, opt-einsum, keras-preprocessing, tensorflow-estimator, markdown, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, requests-oauthlib, google-auth-oauthlib, werkzeug, grpcio, tensorboard-plugin-wit, tensorboard, gast, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "Successfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.22.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TemporalTransformer import Hopper\n",
    "from TemporalTransformer import Prepper\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS i_ids;\n",
      "\n",
      "CREATE TABLE i_ids (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\tPRIMARY KEY (i_id)\n",
      ");\n",
      "\n",
      "DROP TABLE IF EXISTS i_windows;\n",
      "\n",
      "CREATE TABLE i_windows (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\ti_st INTEGER NOT NULL,\n",
      "\ti_et INTEGER NOT NULL,\n",
      "\tPRIMARY KEY (i_id),\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "DROP TABLE IF EXISTS i_partitions;\n",
      "\n",
      "CREATE TABLE i_partitions (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\tpartition TEXT NOT NULL,\n",
      "\tPRIMARY KEY (i_id),\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "DROP TABLE IF EXISTS i_relcal;\n",
      "\n",
      "CREATE TABLE i_relcal (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\ti_st INTEGER NOT NULL,\n",
      "\ti_et INTEGER NOT NULL\n",
      ");\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "h = Hopper.dbms(verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LISCENSE.txt    README.md             \u001b[0m\u001b[01;34mTemporalTransformer.egg-info\u001b[0m/\r\n",
      "MANIFEST.in.md  setup.py\r\n",
      "\u001b[01;34m__pycache__\u001b[0m/    \u001b[01;34mTemporalTransformer\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS characteristics_0;\n",
      "\n",
      "CREATE TABLE characteristics_0 (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\tage REAL NOT NULL,\n",
      "\tsex TEXT NOT NULL,\n",
      "\teth TEXT NOT NULL,\n",
      "\tbin_ldc TEXT NOT NULL,\n",
      "\tjob TEXT NOT NULL,\n",
      "\tPRIMARY KEY (i_id),\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "INSERT OR IGNORE INTO i_ids (i_id) VALUES (?);\n",
      "\n",
      "INSERT  INTO characteristics_0 (i_id, age, sex, eth, bin_ldc, job) VALUES (?, ?, ?, ?, ?, ?);\n",
      "\n",
      "CREATE INDEX i_index_characteristics_0_i_id ON characteristics_0(i_id);\n",
      "\n",
      "CREATE INDEX i_index_characteristics_0_age ON characteristics_0(age);\n",
      "\n",
      "CREATE INDEX i_index_characteristics_0_sex ON characteristics_0(sex);\n",
      "\n",
      "CREATE INDEX i_index_characteristics_0_eth ON characteristics_0(eth);\n",
      "\n",
      "CREATE INDEX i_index_characteristics_0_bin_ldc ON characteristics_0(bin_ldc);\n",
      "\n",
      "CREATE INDEX i_index_characteristics_0_job ON characteristics_0(job);\n",
      "\n",
      "10000 rows loaded\n",
      "\n",
      "DROP TABLE IF EXISTS samples_0;\n",
      "\n",
      "CREATE TABLE samples_0 (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\ti_st INTEGER NOT NULL,\n",
      "\ti_et INTEGER NOT NULL,\n",
      "\tSBP REAL NOT NULL,\n",
      "\tDBP REAL NOT NULL,\n",
      "\ttype TEXT NOT NULL,\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "INSERT OR IGNORE INTO i_ids (i_id) VALUES (?);\n",
      "\n",
      "INSERT  INTO samples_0 (i_id, i_st, i_et, SBP, DBP, type) VALUES (?, ?, ?, ?, ?, ?);\n",
      "\n",
      "CREATE INDEX i_index_samples_0_i_id ON samples_0(i_id);\n",
      "\n",
      "CREATE INDEX i_index_samples_0_i_st ON samples_0(i_st);\n",
      "\n",
      "CREATE INDEX i_index_samples_0_i_et ON samples_0(i_et);\n",
      "\n",
      "CREATE INDEX i_index_samples_0_SBP ON samples_0(SBP);\n",
      "\n",
      "CREATE INDEX i_index_samples_0_DBP ON samples_0(DBP);\n",
      "\n",
      "CREATE INDEX i_index_samples_0_type ON samples_0(type);\n",
      "\n",
      "200000 rows loaded\n",
      "\n",
      "DROP TABLE IF EXISTS samples_1;\n",
      "\n",
      "CREATE TABLE samples_1 (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\ti_st INTEGER NOT NULL,\n",
      "\ti_et INTEGER NOT NULL,\n",
      "\ttmps REAL NOT NULL,\n",
      "\thrs REAL NOT NULL,\n",
      "\tsbps REAL NOT NULL,\n",
      "\trrs REAL NOT NULL,\n",
      "\tdbps REAL NOT NULL,\n",
      "\tsats REAL NOT NULL,\n",
      "\twgts REAL NOT NULL,\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "INSERT OR IGNORE INTO i_ids (i_id) VALUES (?);\n",
      "\n",
      "INSERT  INTO samples_1 (i_id, i_st, i_et, tmps, hrs, sbps, rrs, dbps, sats, wgts) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_i_id ON samples_1(i_id);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_i_st ON samples_1(i_st);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_i_et ON samples_1(i_et);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_tmps ON samples_1(tmps);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_hrs ON samples_1(hrs);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_sbps ON samples_1(sbps);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_rrs ON samples_1(rrs);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_dbps ON samples_1(dbps);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_sats ON samples_1(sats);\n",
      "\n",
      "CREATE INDEX i_index_samples_1_wgts ON samples_1(wgts);\n",
      "\n",
      "200000 rows loaded\n",
      "\n",
      "DROP TABLE IF EXISTS samples_2;\n",
      "\n",
      "CREATE TABLE samples_2 (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\ti_st INTEGER NOT NULL,\n",
      "\ti_et INTEGER NOT NULL,\n",
      "\tdx TEXT NOT NULL,\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "INSERT OR IGNORE INTO i_ids (i_id) VALUES (?);\n",
      "\n",
      "INSERT  INTO samples_2 (i_id, i_st, i_et, dx) VALUES (?, ?, ?, ?);\n",
      "\n",
      "CREATE INDEX i_index_samples_2_i_id ON samples_2(i_id);\n",
      "\n",
      "CREATE INDEX i_index_samples_2_i_st ON samples_2(i_st);\n",
      "\n",
      "CREATE INDEX i_index_samples_2_i_et ON samples_2(i_et);\n",
      "\n",
      "CREATE INDEX i_index_samples_2_dx ON samples_2(dx);\n",
      "\n",
      "200000 rows loaded\n",
      "\n",
      "DROP TABLE IF EXISTS samples_3;\n",
      "\n",
      "CREATE TABLE samples_3 (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\ti_st INTEGER NOT NULL,\n",
      "\ti_et INTEGER NOT NULL,\n",
      "\toutcome TEXT NOT NULL,\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "INSERT OR IGNORE INTO i_ids (i_id) VALUES (?);\n",
      "\n",
      "INSERT  INTO samples_3 (i_id, i_st, i_et, outcome) VALUES (?, ?, ?, ?);\n",
      "\n",
      "CREATE INDEX i_index_samples_3_i_id ON samples_3(i_id);\n",
      "\n",
      "CREATE INDEX i_index_samples_3_i_st ON samples_3(i_st);\n",
      "\n",
      "CREATE INDEX i_index_samples_3_i_et ON samples_3(i_et);\n",
      "\n",
      "CREATE INDEX i_index_samples_3_outcome ON samples_3(outcome);\n",
      "\n",
      "10000 rows loaded\n",
      "\n",
      "DROP TABLE IF EXISTS samples_4;\n",
      "\n",
      "CREATE TABLE samples_4 (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\ti_st INTEGER NOT NULL,\n",
      "\ti_et INTEGER NOT NULL,\n",
      "\tq TEXT NOT NULL,\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "INSERT OR IGNORE INTO i_ids (i_id) VALUES (?);\n",
      "\n",
      "INSERT  INTO samples_4 (i_id, i_st, i_et, q) VALUES (?, ?, ?, ?);\n",
      "\n",
      "CREATE INDEX i_index_samples_4_i_id ON samples_4(i_id);\n",
      "\n",
      "CREATE INDEX i_index_samples_4_i_st ON samples_4(i_st);\n",
      "\n",
      "CREATE INDEX i_index_samples_4_i_et ON samples_4(i_et);\n",
      "\n",
      "CREATE INDEX i_index_samples_4_q ON samples_4(q);\n",
      "\n",
      "10000 rows loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tc = Hopper.table_config(\"characteristics_0\", \n",
    "                  [\"age\", \"sex\", \"eth\", \"bin_ldc\", \"job\"], \n",
    "                  [\"real\", \"bin\", \"ldc\", \"ldc\", \"hdc\"],\n",
    "                  has_times=False,\n",
    "                  primary_key=True)\n",
    "h.create_fvm_with_csv(_tc, \"/home/jovyan/work/TemporalTransformer/notebooks/data/characteristics_0.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "_tc = Hopper.table_config(\"samples_0\", \n",
    "                  [\"SBP\", \"DBP\", \"type\"], \n",
    "                  [\"real\", \"real\", \"ldc\"], \n",
    "                  has_times=True,\n",
    "                  primary_key=False)\n",
    "h.create_fvm_with_csv(_tc, \"/home/jovyan/work/TemporalTransformer/notebooks/data/samples_0.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "_tc = Hopper.table_config(\"samples_1\", \n",
    "                  [\"tmps\", \"hrs\", \"sbps\", \"rrs\", \"dbps\", \"sats\", \"wgts\"],\n",
    "                  [\"real\", \"real\", 'real', \"real\", \"real\", \"real\", \"real\"], \n",
    "                  has_times=True,\n",
    "                  primary_key=False)\n",
    "h.create_fvm_with_csv(_tc, \"/home/jovyan/work/TemporalTransformer/notebooks/data/samples_1.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "_tc = Hopper.table_config(\"samples_2\", \n",
    "                  [\"dx\"],\n",
    "                  [\"hdc\"], \n",
    "                  has_times=True,\n",
    "                  primary_key=False)\n",
    "h.create_fvm_with_csv(_tc, \"/home/jovyan/work/TemporalTransformer/notebooks/data/samples_2.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "_tc = Hopper.table_config(\"samples_3\", \n",
    "                  [\"outcome\"],\n",
    "                  [\"bin\"], \n",
    "                  has_times=True,\n",
    "                  primary_key=False)\n",
    "h.create_fvm_with_csv(_tc, \"/home/jovyan/work/TemporalTransformer/notebooks/data/samples_3.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "_tc = Hopper.table_config(\"samples_4\", \n",
    "                  [\"q\"],\n",
    "                  [\"bin\"], \n",
    "                  has_times=True,\n",
    "                  primary_key=False)\n",
    "h.create_fvm_with_csv(_tc, \"/home/jovyan/work/TemporalTransformer/notebooks/data/samples_4.csv\", delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT  INTO i_windows (i_id, i_st, i_et) VALUES (?, ?, ?);\n",
      "\n",
      "gen_range_table_sql called: af=None, bf=None\n",
      "INSERT OR IGNORE INTO i_windows \n",
      "SELECT * FROM (\n",
      "\tSELECT i_id, 0 AS i_st, 5 AS i_et\n",
      "\tFROM (\n",
      "\t    SELECT i_id, i_st, i_et\n",
      "\t\tFROM samples_0\n",
      "\t\tUNION\n",
      "\t\t\tSELECT i_id, i_st, i_et\n",
      "\t\tFROM samples_0\n",
      "\t\tUNION\n",
      "\t\t\tSELECT i_id, i_st, i_et\n",
      "\t\tFROM samples_1\n",
      "\t\tUNION\n",
      "\t\t\tSELECT i_id, i_st, i_et\n",
      "\t\tFROM samples_2\n",
      "\t\tUNION\n",
      "\t\t\tSELECT i_id, i_st, i_et\n",
      "\t\tFROM samples_3\n",
      "\t\tUNION\n",
      "\t\t\tSELECT i_id, i_st, i_et\n",
      "\t\tFROM samples_4\n",
      "\t\t\n",
      "\t)\n",
      "\tGROUP BY i_id\n",
      ");\n",
      "\n",
      "INSERT  INTO i_partitions (i_id, partition) VALUES (?, ?);\n",
      "\n",
      "INSERT OR IGNORE INTO i_partitions \n",
      "SELECT * FROM (\n",
      "\tSELECT i_id, CASE \n",
      "\t\t WHEN r <= 0.8 THEN \"train\" \n",
      "\t\t WHEN r <= 0.9 THEN \"dev\" \n",
      "\t\t WHEN r <= 1.0 THEN \"test\" \n",
      "\t\t ELSE \"test\" \n",
      "\tEND\n",
      "\tFROM (SELECT i_id, ABS(RANDOM())/(9223372036854775807.0) AS r FROM i_ids)\n",
      ");\n",
      "\n",
      "CREATE VIEW win_samples_0 AS\n",
      "    SELECT samples_0.i_id,\n",
      "\tMAX(samples_0.i_st, i_windows.i_st) AS i_st,\n",
      "\tMIN(samples_0.i_et, i_windows.i_et) AS i_et,\n",
      "\tSBP,\n",
      "\tDBP,\n",
      "\ttype\n",
      "\tFROM (\n",
      "\t    samples_0 JOIN i_windows ON samples_0.i_id=i_windows.i_id\n",
      "\t)\n",
      "\tWHERE NOT (i_windows.i_et < samples_0.i_st OR\n",
      "\t           i_windows.i_st >= samples_0.i_et)\n",
      "\t\n",
      ";\n",
      "\n",
      "CREATE VIEW win_samples_0 AS\n",
      "    SELECT samples_0.i_id,\n",
      "\tMAX(samples_0.i_st, i_windows.i_st) AS i_st,\n",
      "\tMIN(samples_0.i_et, i_windows.i_et) AS i_et,\n",
      "\tSBP,\n",
      "\tDBP,\n",
      "\ttype\n",
      "\tFROM (\n",
      "\t    samples_0 JOIN i_windows ON samples_0.i_id=i_windows.i_id\n",
      "\t)\n",
      "\tWHERE NOT (i_windows.i_et < samples_0.i_st OR\n",
      "\t           i_windows.i_st >= samples_0.i_et)\n",
      "\t\n",
      ";\n",
      "\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "table win_samples_0 already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1c3822b70700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m h.dew_it(fit_normalization_via_sql_qds=False,\n\u001b[0m\u001b[1;32m      2\u001b[0m          \u001b[0mdefault_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          default_last=5)\n",
      "\u001b[0;32m~/work/TemporalTransformer/Hopper.py\u001b[0m in \u001b[0;36mdew_it\u001b[0;34m(self, after_first, before_last, default_first, default_last, fit_normalization_via_sql_qds)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_windows_views\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_partition_views_prior_to_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/TemporalTransformer/Hopper.py\u001b[0m in \u001b[0;36mcreate_windows_views\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_fvm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_fvms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             \u001b[0m_fvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_windows_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;31m#TODO: check that this is actually equally allocating between\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/TemporalTransformer/Hopper.py\u001b[0m in \u001b[0;36mcreate_windows_view\u001b[0;34m(self, from_view)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mfrom_tc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mnew_col_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_new_col_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_tc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_win\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_tc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"win\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_col_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_substatement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindows_view_sql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/TemporalTransformer/Hopper.py\u001b[0m in \u001b[0;36m_create_view\u001b[0;34m(self, from_tc, new_view, new_col_info, sql_substatement, create_view_sql)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mcreate_view_sql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_view_sql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_substatement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_substatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_vn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_man\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_view_sql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         new_tc = table_config(new_vn, new_cns, new_types,\n",
      "\u001b[0;32m~/work/TemporalTransformer/sqlite_utils.py\u001b[0m in \u001b[0;36mexecute_sql\u001b[0;34m(self, sql_stmt, param, many)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_stmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmany\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         execute_sql(self.cur, sql_stmt,\n\u001b[0m\u001b[1;32m     28\u001b[0m                     verbose=self.verbose, param=param, many=many)\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/TemporalTransformer/sqlite_utils.py\u001b[0m in \u001b[0;36mexecute_sql\u001b[0;34m(cur, sql_stmt, verbose, param, many)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_stmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmany\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: table win_samples_0 already exists"
     ]
    }
   ],
   "source": [
    "h.dew_it(fit_normalization_via_sql_qds=False,\n",
    "         default_first=0,\n",
    "         default_last=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'database': ':memory:',\n",
       " 'verbose': True,\n",
       " 'con': <sqlite3.Connection at 0x7f299f0a56c0>,\n",
       " 'cur': <sqlite3.Cursor at 0x7f299efabf80>,\n",
       " 'cur_man': <TemporalTransformer.sqlite_utils.cursor_manager at 0x7f29cb1b3b50>,\n",
       " 'mn_tc': <TemporalTransformer.Hopper.table_config at 0x7f29f0754eb0>,\n",
       " 'mn_dtm': <TemporalTransformer.Hopper.data_table_manager at 0x7f29cb1b38e0>,\n",
       " 'wn_tc': <TemporalTransformer.Hopper.table_config at 0x7f29f074cb80>,\n",
       " 'wn_dtm': <TemporalTransformer.Hopper.data_table_manager at 0x7f29f06df910>,\n",
       " 'pr_tc': <TemporalTransformer.Hopper.table_config at 0x7f29f0740100>,\n",
       " 'pr_dtm': <TemporalTransformer.Hopper.data_table_manager at 0x7f29cae3ffd0>,\n",
       " 'rc_tc': <TemporalTransformer.Hopper.table_config at 0x7f29cae3fe50>,\n",
       " 'rc_dtm': <TemporalTransformer.Hopper.data_table_manager at 0x7f29cae3fe80>,\n",
       " 'windows': True,\n",
       " 'partitions': True,\n",
       " 'relcal_set': False,\n",
       " 'fvms': [<TemporalTransformer.Hopper.flow_view_manager at 0x7f299ef6a460>,\n",
       "  <TemporalTransformer.Hopper.flow_view_manager at 0x7f299ef62400>,\n",
       "  <TemporalTransformer.Hopper.flow_view_manager at 0x7f299e1cf1f0>,\n",
       "  <TemporalTransformer.Hopper.flow_view_manager at 0x7f299e3c1d30>,\n",
       "  <TemporalTransformer.Hopper.flow_view_manager at 0x7f299d813280>,\n",
       "  <TemporalTransformer.Hopper.flow_view_manager at 0x7f299d813220>,\n",
       "  <TemporalTransformer.Hopper.flow_view_manager at 0x7f29f0754550>,\n",
       "  <TemporalTransformer.Hopper.flow_view_manager at 0x7f299d813ac0>,\n",
       "  <TemporalTransformer.Hopper.flow_view_manager at 0x7f299d8134f0>,\n",
       "  <TemporalTransformer.Hopper.flow_view_manager at 0x7f299d813df0>]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbms_save_dict = vars(h)\n",
    "dbms_save_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characteristics_0\n",
      "characteristics_0\n",
      "characteristics_0\n",
      "samples_0\n",
      "characteristics_0\n",
      "samples_0\n",
      "samples_1\n",
      "samples_2\n",
      "samples_3\n",
      "samples_4\n"
     ]
    }
   ],
   "source": [
    "for fvm in dbms_save_dict[\"fvms\"]:\n",
    "    print(fvm.tc.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'flow_view_manager' object has no attribute 'nrm_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-64a3a76b7651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfvm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdbms_save_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fvms\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'flow_view_manager' object has no attribute 'nrm_config'"
     ]
    }
   ],
   "source": [
    "for fvm in dbms_save_dict[\"fvms\"]:\n",
    "    print(fvm.nrm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'flow_view_manager' object has no attribute 'nrm_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6fdf28b084a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     fvm_save_dict = {\"tc\": fvm_tc_save_dict,\n\u001b[1;32m      6\u001b[0m                      \u001b[0;34m\"filter_config\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                      \"nrm_config\": fvm.nrm_config}\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfvm_save_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfvm_save_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'flow_view_manager' object has no attribute 'nrm_config'"
     ]
    }
   ],
   "source": [
    "fvm_save_dicts = []\n",
    "for fvm in dbms_save_dict[\"fvms\"]:\n",
    "    \n",
    "    fvm_tc_save_dict = fvm.tc._get_save_dict()\n",
    "    fvm_save_dict = {\"tc\": fvm_tc_save_dict,\n",
    "                     \"filter_config\": fvm.filter_config,\n",
    "                     \"nrm_config\": fvm.nrm_config}\n",
    "    \n",
    "    fvm_save_dicts.append(fvm_save_dict)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fvm_save_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS i_ids;\n",
      "\n",
      "CREATE TABLE i_ids (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\tPRIMARY KEY (i_id)\n",
      ");\n",
      "\n",
      "DROP TABLE IF EXISTS i_windows;\n",
      "\n",
      "CREATE TABLE i_windows (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\ti_st INTEGER NOT NULL,\n",
      "\ti_et INTEGER NOT NULL,\n",
      "\tPRIMARY KEY (i_id),\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "DROP TABLE IF EXISTS i_partitions;\n",
      "\n",
      "CREATE TABLE i_partitions (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\tpartition TEXT NOT NULL,\n",
      "\tPRIMARY KEY (i_id),\n",
      "\tFOREIGN KEY (i_id) REFERENCES i_ids (i_id)\n",
      ");\n",
      "\n",
      "DROP TABLE IF EXISTS i_relcal;\n",
      "\n",
      "CREATE TABLE i_relcal (\n",
      "\ti_id TEXT NOT NULL,\n",
      "\ti_st INTEGER NOT NULL,\n",
      "\ti_et INTEGER NOT NULL\n",
      ");\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = Hopper.dbms(verbose=verbose)\n",
    "\n",
    "for fvm_save_dict in fvm_save_dicts:\n",
    "    #_tc = Hopper.table_config.load_save_dict(fvm_save_dict[\"tc\"])\n",
    "    _tc = Hopper.table_config(**fvm_save_dict[\"tc\"])\n",
    "    print(vars(_tc))\n",
    "    k._create_fvm(_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'characteristics_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6740b915a7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_fp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_name_fp_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfvm_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_csv_to_fvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'characteristics_0'"
     ]
    }
   ],
   "source": [
    "table_name_fp_dict = {\"characteristics_0\": \"data/characteristics_0.csv\",\n",
    "                      \"samples_0\": \"data/samples_0.csv\",\n",
    "                      \"samples_1\": \"data/samples_1.csv\",\n",
    "                      \"samples_2\": \"data/samples_2.csv\",\n",
    "                      \"samples_3\": \"data/samples_3.csv\",\n",
    "                      \"samples_4\": \"data/samples_4.csv\",\n",
    "                     }\n",
    "\n",
    "for table_name, data_fp in table_name_fp_dict.items():\n",
    "    fvm = k.fvm_lookup[table_name]\n",
    "    k._csv_to_fvm(fvm, data_fp, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "after_first=None\n",
    "before_last=None\n",
    "default_first=None\n",
    "default_last=None\n",
    "fit_normalization_via_sql_qds=False\n",
    "\n",
    "\n",
    "if not k.windows:\n",
    "    k.set_windows(data=[],\n",
    "                  after_first=after_first,\n",
    "                  before_last=before_last,\n",
    "                  default_first=default_first,\n",
    "                  default_last=default_last)\n",
    "    \n",
    "if not k.partitions:\n",
    "    k.set_partitions(data=[])\n",
    "    \n",
    "k.create_windows_views()\n",
    "k.create_partition_views_prior_to_filter()\n",
    "\n",
    "#self.fit_filter() -> set filter\n",
    "for fvm_save_dict in fvm_save_dicts:\n",
    "    fvm_name = fvm_save_dict[\"tc\"][\"name\"]\n",
    "    fvm_filter_config = fvm_save_dict[\"filter_config\"]\n",
    "    #print(fvm_name, fvm_filter_config)\n",
    "    \n",
    "    fvm = k.fvm_lookup[fvm_name]\n",
    "    fvm.filter_config = fvm_filter_config\n",
    "\n",
    "k.set_relcal()\n",
    "k.fil_agg()\n",
    "\n",
    "k.create_partition_views_prior_to_normalization() \n",
    "\n",
    "#self.fit_normalization(via_sql_qds=fit_normalization_via_sql_qds) -> set nrm\n",
    "for fvm_save_dict in fvm_save_dicts:\n",
    "    fvm_name = fvm_save_dict[\"tc\"][\"name\"]\n",
    "    fvm_nrm_config = fvm_save_dict[\"nrm_config\"]\n",
    "    #print(fvm_name, fvm_filter_config)\n",
    "    \n",
    "    fvm = k.fvm_lookup[fvm_name]\n",
    "    fvm.nrm_config = fvm_nrm_config\n",
    "    \n",
    "k.normalize()\n",
    "k.create_final_partition_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfp_h = Prepper.tf_prepper(h)\n",
    "tfp_h.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp_k = Prepper.tf_prepper(h)\n",
    "tfp_k.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _i, _j in zip(tfp_h.features, tfp_k.features):\n",
    "    if _i != _j:\n",
    "        print(_i, _j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window, Filter, Transform, Aggregate, Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h.dew_it(after_first=35)\n",
    "h.dew_it(fit_normalization_via_sql_qds=False,\n",
    "         default_first=0,\n",
    "         default_last=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfp = Prepper.tf_prepper(h)\n",
    "tfp.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Prepper to Desired Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp.fit(offsets=[0, 1, 2, 4, 5], label_fns=[\"samples_3/avg_outcome\", ], partition=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d0 = tf.keras.layers.Dense(units=32, name=\"encode\")\n",
    "r0 = tf.keras.layers.LSTM(units=32,return_sequences=True, name=\"RNN_0\")\n",
    "r1 = tf.keras.layers.LSTM(units=16,return_sequences=True, name=\"RNN_1\")\n",
    "\n",
    "models[\"all\"] = tfp.build_model(middle_layer_list=[d0, r0, r1])\n",
    "models[\"all\"].compile(loss=\"binary_crossentropy\")\n",
    "models[\"all\"].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = tf.keras.layers.Dense(units=32, name=\"encode\")\n",
    "r0 = tf.keras.layers.LSTM(units=32,return_sequences=True, name=\"RNN_0\")\n",
    "r1 = tf.keras.layers.LSTM(units=16,return_sequences=True, name=\"RNN_1\")\n",
    "\n",
    "models[\"noWS\"] = tfp.build_model(middle_layer_list=[d0, r0, r1], ignore_fns=['samples_4/avg_q', 'samples_4/count'])\n",
    "models[\"noWS\"].compile(loss=\"binary_crossentropy\")\n",
    "models[\"noWS\"].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = tfp.transform_to_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models[\"all\"].fit(ds[\"train\"], epochs=1, validation_data=ds[\"dev\"])\n",
    "#final_model.fit(ds[\"test\"], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"noWS\"].fit(ds[\"train\"], epochs=1, validation_data=ds[\"dev\"])\n",
    "#final_wo_model.fit(ds[\"test\"], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Individual Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [\"00000\", \"00001\"]:\n",
    "    e = Prepper.entity(idx, tfp)\n",
    "    e.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [\"00000\", \"00001\"]:\n",
    "    e = Prepper.entity(idx, tfp)\n",
    "    print(idx)\n",
    "    for model_name, model in models.items():\n",
    "        print(model_name)\n",
    "        e.predict(model)\n",
    "        e.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operating Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "partitions = [\"train\", \"dev\", \"test\"]\n",
    "pop_res = {partition: {} for partition in partitions}\n",
    "\n",
    "for partition in partitions:\n",
    "    for model_name, model in models.items():\n",
    "        print(partition, model_name)\n",
    "        pop_res[partition][model_name] = \\\n",
    "        Prepper.population(ds[partition], tfp, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    pop_res[\"train\"][model_name].roc_curves()\n",
    "    pop_res[\"train\"][model_name].pr_curves()\n",
    "    pop_res[\"train\"][model_name].calibration_curves()\n",
    "    pop_res[\"train\"][model_name].calibration_curves(n_bins=2)\n",
    "    pop_res[\"train\"][model_name].calibration_curves(quantile=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    pop_res[\"dev\"][model_name].roc_curves()\n",
    "    pop_res[\"dev\"][model_name].pr_curves()\n",
    "    pop_res[\"dev\"][model_name].calibration_curves()\n",
    "    pop_res[\"dev\"][model_name].calibration_curves(quantile=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    pop_res[\"test\"][model_name].roc_curves()\n",
    "    pop_res[\"test\"][model_name].pr_curves()\n",
    "    pop_res[\"test\"][model_name].calibration_curves()\n",
    "    pop_res[\"test\"][model_name].calibration_curves(quantile=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Individual Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e = Prepper.entity(\"00001\", tfp)\n",
    "e.print_labels()\n",
    "e.plot()\n",
    "e.predict(models[\"all\"])\n",
    "e.print_labels()\n",
    "e.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Feature Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    pop_res[\"test\"][model_name].roc_curves()\n",
    "    pop_res[\"test\"][model_name].pr_curves()\n",
    "    pop_res[\"test\"][model_name].calibration_curves()\n",
    "    pop_res[\"test\"][model_name].calibration_curves(quantile=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    pop_res[\"test\"][model_name].calc_weights(lambda x: 1/x)\n",
    "    pop_res[\"test\"][model_name].roc_curves()\n",
    "    pop_res[\"test\"][model_name].pr_curves()\n",
    "    pop_res[\"test\"][model_name].calibration_curves()\n",
    "    pop_res[\"test\"][model_name].calibration_curves(quantile=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    pop_res[\"test\"][model_name].calc_weights(lambda x: math.log(x)/x)\n",
    "    pop_res[\"test\"][model_name].roc_curves()\n",
    "    pop_res[\"test\"][model_name].pr_curves()\n",
    "    pop_res[\"test\"][model_name].calibration_curves()\n",
    "    pop_res[\"test\"][model_name].calibration_curves(quantile=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pop_res[\"test\"][\"all\"].pred[\"samples_3/avg_outcome\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pop_res[\"test\"][\"all\"].weights[\"samples_3/avg_outcome\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=\"00000\"\n",
    "s_X, s_Y = tfp.get_specific_XY(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_Y_h = models[\"all\"].predict(s_X)\n",
    "\n",
    "for i, lb in enumerate(tfp.label_fns):\n",
    "    print(s_Y_h[i].shape, s_Y[lb].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "for fn, v in s_X.items():\n",
    "    print(fn, \"\\n\", np.squeeze(np.array(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ds[\"train\"].take(1):\n",
    "    x, y = i\n",
    "    for k, v in x.items():\n",
    "        print(k, '\\n', v.shape, '\\n')\n",
    "    \n",
    "    for i, _y in enumerate(y):\n",
    "        print(\"y[%s]\" %(i), '\\n', _y.shape, '\\n')\n",
    "\n",
    "print(\"Avg y value: \", tf.reduce_mean(y).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
